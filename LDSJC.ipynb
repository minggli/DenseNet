{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducing DenseNet\n",
    "  \n",
    "  \n",
    "  \n",
    "Ming Li  \n",
    "Data Scientist  \n",
    "Contributor to pandas, scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<figure>\n",
    "<center><img src=\"./images/f838717a-6ad1-11e6-9391-f0906c80bc1d.jpg\" width=\"640\">\n",
    "<figcaption>A single Dense Block of 5 layers.</figcaption></center></figure>\n",
    "\n",
    "* Novel and simple connectivity pattern to increase network connections to $\\frac{ùêø(ùêø+1)}{2}$\n",
    "* Feature reuse.\n",
    "* Parameters and computation efficient.\n",
    "* Outperform current state-of-the-art results across various benchmarks.\n",
    "* Easy and efficient* implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution\n",
    "In continuous domain of $\\tau$, convolution is defined as:  \n",
    "$$(f * g)(\\tau) = \\int_{0}^{t} f(\\tau) g(\\tau - t) d\\tau$$\n",
    "In discrete coordinate space $[h, w]$, this is equivalently defined as:\n",
    "$$(f * g)[h, w] = \\sum_{i}\\sum_{j} f(h, w)g(h - i, w - j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f = np.array([[1, 1, 1, 0, 0], [0, 1, 1, 1, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 0], [0, 1, 1, 0, 0]])\n",
    "g = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<figure>\n",
    "<center><img src=\"./images/Convolution_schematic.gif\" width=\"640\">\n",
    "<figcaption>Convolution during Forward Propagation, <a href=http://ufldl.stanford.edu/wiki/index.php/Feature_extraction_using_convolution>source of image</a></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.3 ¬µs ¬± 3.61 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def element_conv():\n",
    "    h, w = 3, 3\n",
    "    element_conv = np.zeros_like(g)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            kernel = f[i:h+i, j:w+j]\n",
    "            element_conv[i, j] = np.sum(kernel * g)\n",
    "    return element_conv\n",
    "\n",
    "%timeit -n 1000 element_conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 3, 4],\n",
       "       [2, 4, 3],\n",
       "       [2, 3, 4]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_conv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Implementation as Matrix Multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.1 ¬µs ¬± 5.48 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def matmul_conv():\n",
    "    h, w = 3, 3\n",
    "    col = np.zeros([9, 9])\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            col[i*w+j] = f[i:h+i, j:w+j].flatten()\n",
    "    return (g.flatten() @ col).reshape(g.shape)\n",
    "\n",
    " \n",
    "%timeit -n 1000 matmul_conv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(element_conv(), matmul_conv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A Dense Block in DenseNet is a block of hidden layers where subsequent layer reuses feature from preceding layers, through concatenation of feature maps along the depth.  \n",
    "\n",
    "More concretely,\n",
    "$$\\begin{align*}\n",
    "x_{l} &= f_{composite}(x_{0})\\\\\n",
    "x_{2} &= f_{composite}([x_{0}, x_{1}])\\\\\n",
    "x_{3} &= f_{composite}([x_{0}, x_{1}, x_{2}])\n",
    "\\end{align*}$$\n",
    "\n",
    "Inspired from \"identity shortcut connection\" in Residual Block from ResNet, a Dense Block enhances connectivity and avoids loss of informaiton by replacing summation with concatenation. Enhanced connectivity allows Jacobians to flow through layers more freely during backpropagation.  \n",
    "  \n",
    "<figure>\n",
    "<center><img src=\"./images/denseblock.png\" width=\"960\">\n",
    "<figcaption>An illustration of Dense Block of 2 layers</figcaption></center>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Composite Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$f_{composite}$ consists of 3 functions inspired from 'pre-activation' in ResNet: Batch Normalization, ReLU, Convolution.\n",
    "\n",
    "<figure>\n",
    "<center><img src=\"./images/pre-activation.png\" width=\"360\">\n",
    "<figcaption>full pre-activation as in ResNet, note that \"weight\" indicates conv</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "**Batch Normalizing Transform** is defined as:  \n",
    "$$\\begin{align}\n",
    "\\hat{x_{i}} &= \\frac{x_{i} - \\mu_{B}}{\\sqrt{\\sigma_{B}^2 + \\epsilon}}\\\\\n",
    "BN(x_{i}; \\gamma, \\beta) &= \\gamma \\hat{x_{i}} + \\beta\n",
    "\\end{align}$$  \n",
    "  \n",
    "It has the benefit of regularization and data augmentation every time a feature map is reused.  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Rectified Linear Unit (ReLU)** is defined as: $$f(x_{i}) = \\max({0, x_{i}})$$  \n",
    "  \n",
    "Apart from faster evaluation than $\\sigma(z) = \\frac{1}{1 + e^{z}}$, $\\frac{\\partial{f(x)}}{\\partial{x}} \\in \\{0, 1\\}$ is also more favourable to $\\frac{\\partial{\\sigma(x)}}{\\partial{x}} \\in [0., 0.25]$ in terms of reduce gradient vanishing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DenseNet.DenseNet import DenseNet as Constructer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (128, 128, 3)\n",
    "KEEP_RATE = .80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn = Constructer(IMAGE_SHAPE,\n",
    "                 num_classes=17,\n",
    "                 keep_prob=KEEP_RATE,\n",
    "                 growth=32,\n",
    "                 bottleneck=4,\n",
    "                 compression=.5)\n",
    "is_train = dn.is_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet(class_balance=False, l2_norm=False):\n",
    "    \"\"\"DenseNet-BC 121\"\"\"\n",
    "    global prediction, loss, train_step, accuracy, saver, is_train\n",
    "\n",
    "    init_conv = dn.add_conv_layer(\n",
    "                            image_feed,\n",
    "                            [[7, 7, IMAGE_SHAPE[-1], 2 * dn._k], [2 * dn._k]],\n",
    "                            bn=False)\n",
    "    init_pool = dn.add_pooling_layer(init_conv, kernel_size=[1, 3, 3, 1])\n",
    "    dense_block_1 = dn.add_dense_block(init_pool, L=6)\n",
    "    transition_layer_1 = dn.add_transition_layer(dense_block_1)\n",
    "    dense_block_2 = dn.add_dense_block(transition_layer_1, L=12)\n",
    "    transition_layer_2 = dn.add_transition_layer(dense_block_2)\n",
    "    dense_block_3 = dn.add_dense_block(transition_layer_2, L=24)\n",
    "    transition_layer_3 = dn.add_transition_layer(dense_block_3)\n",
    "    dense_block_4 = dn.add_dense_block(transition_layer_3, L=16)\n",
    "    global_pool = dn.add_global_average_pool(dense_block_4)\n",
    "    dim = int(global_pool.get_shape()[-1])\n",
    "    dense_layer_1 = dn.add_dense_layer(global_pool, [[dim, 1000], [1000]],\n",
    "                                       bn=False)\n",
    "    drop_out_1 = dn.add_drop_out_layer(dense_layer_1)\n",
    "    logits = dn.add_read_out_layer(drop_out_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
