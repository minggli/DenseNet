{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducing DenseNet\n",
    "  \n",
    "  \n",
    "  \n",
    "Ming Li  \n",
    "Data Scientist  \n",
    "Contributor to pandas, scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<figure>\n",
    "<center><img src=\"./images/f838717a-6ad1-11e6-9391-f0906c80bc1d.jpg\" width=\"640\">\n",
    "<figcaption>A single Dense Block of 5 layers.</figcaption></center></figure>\n",
    "\n",
    "* Novel and simple connectivity pattern to increase network connections to $\\frac{ùêø(ùêø+1)}{2}$\n",
    "* Feature reuse.\n",
    "* Parameters and computation efficient.\n",
    "* Outperform current state-of-the-art results across various benchmarks.\n",
    "* Easy and efficient* implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution\n",
    "In continuous domain of $\\tau$, convolution is defined as:  \n",
    "  \n",
    "$$(f * g)(\\tau) = \\int_{0}^{t} f(\\tau) g(\\tau - t) d\\tau$$\n",
    "In discrete coordinate space $[h, w]$, this is equivalently defined as:  \n",
    "  \n",
    "$$(f * g)[h, w] = \\sum_{i}\\sum_{j} f(h, w)g(h - i, w - j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f = np.array([[1, 1, 1, 0, 0], [0, 1, 1, 1, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 0], [0, 1, 1, 0, 0]])\n",
    "g = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<figure>\n",
    "<center><img src=\"./images/Convolution_schematic.gif\" width=\"640\">\n",
    "<figcaption><font size=\"-1\">Convolution during Forward Propagation, <a href=http://ufldl.stanford.edu/wiki/index.php/Feature_extraction_using_convolution>source of image</a></font></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.6 ¬µs ¬± 17.2 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def element_conv():\n",
    "    h, w = 3, 3\n",
    "    element_conv = np.zeros_like(g)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            kernel = f[i:h+i, j:w+j]\n",
    "            element_conv[i, j] = np.sum(kernel * g)\n",
    "    return element_conv\n",
    "\n",
    "%timeit -n 1000 element_conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 3, 4],\n",
       "       [2, 4, 3],\n",
       "       [2, 3, 4]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_conv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Implementation as Matrix Multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.8 ¬µs ¬± 10 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def matmul_conv():\n",
    "    h, w = 3, 3\n",
    "    col = np.zeros([9, 9])\n",
    "    for i, j in product(range(h), range(w)):\n",
    "        col[i*w+j] = f[i:h+i, j:w+j].flatten()\n",
    "    return (g.flatten() @ col).reshape(g.shape)\n",
    "\n",
    "%timeit -n 1000 matmul_conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check identity\n",
    "assert np.allclose(element_conv(), matmul_conv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A Dense Block in DenseNet is a block of hidden layers where subsequent layer reuses feature from preceding layers, through concatenation of feature maps along the depth.  \n",
    "\n",
    "More concretely,\n",
    "$$\\begin{align*}\n",
    "x_{l} &= f_{composite}(x_{0})\\\\\n",
    "x_{2} &= f_{composite}([x_{0}, x_{1}])\\\\\n",
    "\\dots\\\\\n",
    "x_{l} &= f_{composite}([x_{0}, x_{1}, x_{2}, \\dots, x_{l-1}])\n",
    "\\end{align*}$$\n",
    "\n",
    "- inspired by \"identity shortcut connection\" in ResNet\n",
    "- avoids loss of informaiton by replacing summation with concatenation.\n",
    "- dense connectivity allows Jacobians to flow through layers more freely during backpropagation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<figure>\n",
    "<center><img src=\"./images/denseblock.png\" width=\"960\">\n",
    "<figcaption><font size=\"-1\">An illustration of Dense Block of 2 layers</font></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Composite Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$f_{composite}$ consists of 3 functions inspired from 'pre-activation' in ResNet: Batch Normalization, ReLU, Convolution.\n",
    "\n",
    "<figure>\n",
    "<center><img src=\"./images/pre-activation.png\" width=\"360\">\n",
    "<figcaption><font size=\"-1\">full pre-activation as in ResNet, note that \"weight\" indicates conv</font></figcaption></center>\n",
    "</figure>\n",
    "  \n",
    "  \n",
    "**Batch Normalizing Transform** is defined as:  \n",
    "$$\\begin{align}\n",
    "\\hat{x_{i}} &= \\frac{x_{i} - \\mu_{B}}{\\sqrt{\\sigma_{B}^2 + \\epsilon}}\\\\\n",
    "BN(x_{i}; \\gamma, \\beta) &= \\gamma \\hat{x_{i}} + \\beta\n",
    "\\end{align}$$  \n",
    "  \n",
    "It has the benefit of regularization and data augmentation every time a feature map is reused.  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Rectified Linear Unit (ReLU)** is defined as: $$f(x) = \\max({0, x})$$  \n",
    "  \n",
    "- faster evaluation than sigmoid function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "- $\\frac{\\partial{f(x)}}{\\partial{x}} \\in \\{0, 1\\}$ is favourable to $\\frac{\\partial{\\sigma(x)}}{\\partial{x}} \\in [0., 0.25]$ in regards to reducing gradient vanishing.\n",
    "- may cause 'dead neurons', unable to recover due to $\\forall x \\in (-\\infty, 0), \\ \\frac{\\partial{f(x)}}{\\partial{x}} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense Connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- direct connections less prone to gradient vanishing, allowing DenseNet to scale\n",
    "- implicit data augmentation from feature resuse and Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<figure>\n",
    "<center><img src=\"./images/denseconnectivity.png\" width=\"960\">\n",
    "<figcaption><font size=\"-1\">A DenseNet with multiple dense blocks.</font></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- Huang et al 2016. Densely Connected Convolutional Networks. accessible at: https://arxiv.org/abs/1608.06993\n",
    "- Krizhevsky et al 2012. Deep Convolutional Neural Networks. accessible at : https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "- Simonyan and Zisserman 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. accessible at: https://arxiv.org/pdf/1409.1556.pdf\n",
    "- Szegedy et al 2014. Going Deeper with Convolutions. accessible at https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf\n",
    "- He et al 2015. Deep Residual Learning for Image Recognition. accessible at: https://arxiv.org/abs/1512.03385\n",
    "- He et al 2016. Identity Mappings in Deep Residual Networks. accessible at: https://arxiv.org/abs/1603.05027\n",
    "- Ioffe and Szegedy 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. accessible at: https://arxiv.org/abs/1502.03167\n",
    "- Pleiss et al 2017. Memory-Efficient Implementation of DenseNets. accessible at: https://arxiv.org/abs/1707.06990\n",
    "- Zhang et al 2015. Character-level Convolutional Networks for Text Classification. accessible at: https://arxiv.org/abs/1509.01626\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
