{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducing DenseNet\n",
    "  \n",
    "  \n",
    "  \n",
    "Ming Li  \n",
    "  \n",
    "  \n",
    "Data Scientist  \n",
    "Open Source Contributor (pandas, scikit-learn, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<figure>\n",
    "<center><img src=\"./images/f838717a-6ad1-11e6-9391-f0906c80bc1d.jpg\" width=\"640\">\n",
    "<figcaption><font size=\"-1\">A single Dense Block of 5 layers. Huang et al 2016.</font></figcaption></center></figure>\n",
    "\n",
    "* Novel connectivity pattern to increase network connections to quadratic in relation to layers: $\\frac{ùêø(ùêø+1)}{2}$\n",
    "* Feature reuse.\n",
    "* Parameters and computation efficient.\n",
    "* Outperform current state-of-the-art results across various benchmarks.\n",
    "* Easy and efficient (Pleiss et al 2017) implementation available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution\n",
    "In continuous domain of $\\tau$, convolution is defined as:  \n",
    "  \n",
    "$$(f * g)(\\tau) = \\int_{0}^{t} f(\\tau) g(\\tau - t) d\\tau$$\n",
    "In discrete coordinate space $[h, w]$, this is equivalently defined as:  \n",
    "  \n",
    "$$(f * g)[h, w] = \\sum_{i}\\sum_{j} f(h, w)g(h - i, w - j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f = np.array([[1, 1, 1, 0, 0], [0, 1, 1, 1, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 0], [0, 1, 1, 0, 0]])\n",
    "g = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<figure>\n",
    "<center><img src=\"./images/Convolution_schematic.gif\" width=\"640\">\n",
    "<figcaption><font size=\"-1\">Convolution during Forward Propagation, <a href=http://ufldl.stanford.edu/wiki/index.php/Feature_extraction_using_convolution>source of image</a></font></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.7 ¬µs ¬± 10.2 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def element_conv():\n",
    "    h, w = 3, 3\n",
    "    element_conv = np.zeros_like(g, dtype=np.float32)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            kernel = f[i:h+i, j:w+j]\n",
    "            element_conv[i, j] = np.sum(kernel * g)\n",
    "    return element_conv\n",
    "\n",
    "%timeit -n 1000 element_conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 3., 4.],\n",
       "       [2., 4., 3.],\n",
       "       [2., 3., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_conv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Implementation as Matrix Multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.8 ¬µs ¬± 3.65 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def matmul_conv():\n",
    "    h, w = 3, 3\n",
    "    col = np.zeros([9, 9])\n",
    "    for i, j in product(range(h), range(w)):\n",
    "        col[i*w+j] = f[i:h+i, j:w+j].flatten()\n",
    "    return (g.flatten() @ col).reshape(g.shape)\n",
    "\n",
    "%timeit -n 1000 matmul_conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check identity\n",
    "assert np.allclose(element_conv(), matmul_conv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A Dense Block in DenseNet is a block of hidden layers where subsequent layer reuses feature from preceding layers, through concatenation of feature maps along the depth. More concretely,\n",
    "$$\\begin{align*}\n",
    "x_{l} &= f_{composite}(x_{0})\\\\\n",
    "x_{2} &= f_{composite}([x_{0}, x_{1}])\\\\\n",
    "\\dots\\\\\n",
    "x_{l} &= f_{composite}([x_{0}, x_{1}, x_{2}, \\dots, x_{l-1}])\n",
    "\\end{align*}$$\n",
    "\n",
    "- inspired by \"identity skip connection\" in ResNet\n",
    "- avoids informaiton loss by using concatenation (not summation).\n",
    "- bottleneck and compression techniques to control growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<figure>\n",
    "<center><img src=\"./images/denseblock.png\" width=\"960\">\n",
    "<figcaption><font size=\"-1\">An illustration of Dense Block of 2 layers</font></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Composite Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$f_{composite}$ consists of 3 functions inspired from 'pre-activation' in ResNet: Batch Normalization, ReLU, Convolution, i.e. $$f_{composite}(x) = BN(ReLU(Conv(x)))$$\n",
    "\n",
    "<figure>\n",
    "<center><img src=\"./images/pre-activation.png\" width=\"360\">\n",
    "<figcaption><font size=\"-1\">full pre-activation as in ResNet, note that \"weight\" indicates conv. He et al 2016</font></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Batch Normalizing Transform** is defined as:  \n",
    "$$\\begin{align}\n",
    "\\hat{x_{i}} &= \\frac{x_{i} - \\mu_{B}}{\\sqrt{\\sigma_{B}^2 + \\epsilon}}\\\\\n",
    "BN(x_{i}; \\gamma, \\beta) &= \\gamma \\hat{x_{i}} + \\beta\n",
    "\\end{align}$$  \n",
    "\n",
    "It has the benefit of reducing internal covariance shift (moments of activations), accelerating training, regularizing. In addition, with DenseNet, it produces data augmentation every time a feature map is reused.  \n",
    "\n",
    "<figure>\n",
    "<center><img src=\"./images/bn1.png\" width=\"960\">\n",
    "<figcaption><font size=\"-1\">Test Acurracy and Distribution of Logits over time. Ioffe and Szegedy 2015</font></figcaption></center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Rectified Linear Unit (ReLU)** is defined as: $$f(x) = \\max({0, x})$$  \n",
    "  \n",
    "- faster evaluation than sigmoid function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "- $\\frac{\\partial{f(x)}}{\\partial{x}} \\in \\{0, 1\\}$ is more favourable to $\\frac{\\partial{\\sigma(x)}}{\\partial{x}} \\in [0., 0.25]$ in regards to reducing gradient vanishing.\n",
    "- may cause 'dead neurons', unable to recover due to $\\forall x \\in (-\\infty, 0), \\ \\frac{\\partial{f(x)}}{\\partial{x}} = 0$.\n",
    "\n",
    "\n",
    "<figure>\n",
    "<center><img src=\"./images/saturation.png\" width=\"720\">\n",
    "<figcaption><font size=\"-1\">Saturated layer slowing down learning. Glorot and Bengio et al 2010</font></figcaption></center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense Connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- direct connections allow Jacobians to travel further with reduced gradient vanishing, so deeper network is sensibly possible.\n",
    "- fewer parameters required, less prone to overfitting.\n",
    "- pseudo-augmentation due to feature resuse and Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<figure>\n",
    "<center><img src=\"./images/denseconnectivity.png\" width=\"1440\">\n",
    "<figcaption><font size=\"-1\">A DenseNet with multiple dense blocks.</font></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "a _very_ illustrative example:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\delta{w_1} &= \\frac{\\partial{E_{1}}}{\\partial{w_1}} + \\frac{\\partial{E_{2}}}{\\partial{w_1}} + \\dots \\\\\n",
    "\\delta{w_1} &= \\frac{\\partial{E_{1}}}{\\partial{f(x_1)}}\\frac{\\partial{f(x_1)}}{\\partial{x_1}}\\frac{\\partial{x_1}}{\\partial{w_1}} + \\frac{\\partial{E_{2}}}{\\partial{f([x_1, x_2])}}\\frac{\\partial{f([x_1, x_2])}}{\\partial{x_1}}\\frac{\\partial{x_1}}{\\partial{w_1}} + \\dots\\\\\n",
    "\\text{then}\\\\\n",
    "\\hat{w_1} &= w_1 - \\lambda\\delta{w_1}\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pleiss et al 2017 reduces memory consumption exploiting cheap concatenation and BN operations.\n",
    "- Two pre-allocated shared memory storages for intermediate outputs from concatenation and BN.\n",
    "\n",
    "<figure>\n",
    "<center><img src=\"./images/memory.png\" width=\"960\">\n",
    "<figcaption><font size=\"-1\">efficient implementation. Pleiss et al 2017</font></figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Not Mentioned\n",
    "\n",
    "- bottleneck\n",
    "- transition layers\n",
    "- compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reference\n",
    "\n",
    "- Huang et al 2016. Densely Connected Convolutional Networks. accessible at: https://arxiv.org/abs/1608.06993\n",
    "- Glorot and Bengio et al 2010. Understanding the difficulty of training deep feedforward neural networks. accessible at: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "- He et al 2016. Identity Mappings in Deep Residual Networks. accessible at: https://arxiv.org/abs/1603.05027\n",
    "- Ioffe and Szegedy 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. accessible at: https://arxiv.org/abs/1502.03167\n",
    "- Pleiss et al 2017. Memory-Efficient Implementation of DenseNets. accessible at: https://arxiv.org/abs/1707.06990\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reads\n",
    "- Wan et al 2018. Reconciling Feature-Reuse and Overfitting in DenseNet with Specialized Dropout. accessible at: https://arxiv.org/abs/1810.00091\n",
    "- Zhang et al 2015. Character-level Convolutional Networks for Text Classification. accessible at: https://arxiv.org/abs/1509.01626\n",
    "- He et al 2015. Deep Residual Learning for Image Recognition. accessible at: https://arxiv.org/abs/1512.03385\n",
    "- Krizhevsky et al 2012. Deep Convolutional Neural Networks. accessible at : https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "- Simonyan and Zisserman 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. accessible at: https://arxiv.org/pdf/1409.1556.pdf\n",
    "- Szegedy et al 2014. Going Deeper with Convolutions. accessible at https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf\n",
    "  \n",
    "## Slides\n",
    "https://github.com/minggli/DenseNet"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
